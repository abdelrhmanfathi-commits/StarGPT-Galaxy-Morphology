# ========================================================
# STAR GPT: Galaxy Morphology Classification with Zoobot
# ECCE 635/787 Deep Learning Systems Design Project
# | UAE | November 11, 2025
# @title Optimization Run - 50k Samples - 30 Epochs
# ========================================================

from google.colab import drive
drive.mount('/content/drive')

# =========================================
# SECTION 0: IMPORTS
# =========================================
import os, json, csv, logging, warnings
from pathlib import Path
from datetime import datetime
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# LIGHTNING v2+ → USE lightning.pytorch
import lightning.pytorch as L
from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, Callback
from lightning.pytorch import seed_everything

# ZOOBOT v2.9.0 → CORRECT IMPORT
from zoobot.pytorch.training import finetune
FinetuneableZoobotClassifier = finetune.FinetuneableZoobotClassifier

# TORCHMETRICS
from torchmetrics.classification import ConfusionMatrix, AUROC

# DATASET
from datasets import load_dataset

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("StarGPT")
seed_everything(42)

CLASS_NAMES = ['Smooth', 'Edge-on', 'Barred Spiral', 'Unbarred Spiral', 'Featured', 'Artifact']
CLASS_DICT = {i: name for i, name in enumerate(CLASS_NAMES)}


# =========================================
# SECTION 1: CONFIGURATION (LOCKED — DO NOT CHANGE)
# =========================================
config = {
    "run_name": "baseline_run",
    "description": "ConvNeXt-Base, 50k samples, 30 epochs, full finetuning",
    "use_colab": os.path.exists('/content'),
    "base_dir_colab": '/content/drive/My Drive/Colab_Results', # <-- LIKE THIS
    "base_dir_local": str(Path.home() / 'zoobot_experiments'),
    "max_samples": 50000,           # ← UPDATED TO 50K
    "val_ratio": 0.10,
    "test_ratio": 0.20,
    "random_state": 42,
    "model_name": "hf_hub:mwalmsley/zoobot-encoder-convnext_base",
    "num_classes": 6,
    "dropout_prob": 0.3,
    "batch_size": 64,               # ← UPDATED TO 64
    "learning_rate": 5e-5,
    "layer_decay": 0.8,
    "max_epochs": 10,               # ← UPDATED TO 10
    "early_stopping_patience": 3,
    "training_mode": "full"
}

# Paths
USE_COLAB = config['use_colab']
BASE_DIR = Path(config['base_dir_colab'] if USE_COLAB else config['base_dir_local'])
DATA_DIR = BASE_DIR / 'gz2_data'
SAVE_ROOT = BASE_DIR / 'experiment_runs'
RUN_ID = f"{config['run_name']}_{datetime.now():%Y%m%d_%H%M%S}"
EXP_DIR = SAVE_ROOT / RUN_ID

for p in [BASE_DIR, DATA_DIR, SAVE_ROOT, EXP_DIR]: p.mkdir(parents=True, exist_ok=True)

print(f"Base: {BASE_DIR}\nData: {DATA_DIR}\nSave: {EXP_DIR}")

# Save config
with open(EXP_DIR / "config.json", 'w') as f:
    json.dump(config, f, indent=4)

# =========================================
# SECTION 2: DATA LOADER
# =========================================
def assign_morphology_class(ex):
    s = ex.get('smooth-or-featured-gz2_smooth_fraction', 0.0)
    f = ex.get('smooth-or-featured-gz2_featured-or-disk_fraction', 0.0)
    e = ex.get('disk-edge-on-gz2_yes_fraction', 0.0)
    sp = ex.get('has-spiral-arms-gz2_yes_fraction', 0.0)
    b = ex.get('bar-gz2_yes_fraction', 0.0)
    if s > 0.5: return 0
    elif f > 0.5:
        if e > 0.5: return 1
        elif sp > 0.5: return 2 if b > 0.5 else 3
        else: return 4
    else: return 5

def prepare_gz2_dataset_multiclass(data_dir: Path, max_samples=5000):
    logger.info("Streaming GZ2 dataset...")
    ds = load_dataset("mwalmsley/gz2", split="train", streaming=True)
    os.makedirs(data_dir, exist_ok=True)
    recs = []
    for i, ex in enumerate(ds.take(max_samples)):
        if i % (max_samples // 10) == 0: logger.info(f"Processing {i}/{max_samples}")
        gid = str(ex.get('id_str', f'gal_{i}')).replace('.jpg', '')
        img_path = data_dir / f"{gid}.jpg"
        if 'image' in ex and not img_path.exists():
            try: ex['image'].save(img_path)
            except: continue
        if not img_path.exists(): continue
        cls = assign_morphology_class(ex)
        recs.append({"id_str": gid, "file_loc": str(img_path), "morphology": int(cls)})
    df = pd.DataFrame(recs)
    logger.info(f"Classes:\n{df['morphology'].value_counts().sort_index().rename(CLASS_DICT)}")
    return df

# =========================================
# SECTION 3: LOAD & SPLIT
# =========================================
print("="*80)
print(f"Loading dataset (Max Samples: {config['max_samples']})...")
print("="*80)
full_df = prepare_gz2_dataset_multiclass(DATA_DIR, max_samples=config['max_samples'])
n_samples = min(config['max_samples'], len(full_df))
capped_df = full_df.sample(n=n_samples, random_state=config['random_state']).reset_index(drop=True)

train_val_df, test_df = train_test_split(capped_df, test_size=config['test_ratio'], stratify=capped_df['morphology'], random_state=config['random_state'])
train_df, val_df = train_test_split(train_val_df, test_size=config['val_ratio'], stratify=train_val_df['morphology'], random_state=config['random_state'])

train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print(f"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")
print(f"Leakage: {len(set(train_df.id_str) & set(test_df.id_str))}")

# =========================================
# SECTION 4: DATALOADER + OVERSAMPLING
# =========================================
class CatalogFrameDataset(Dataset):
    def __init__(self, df, label_cols, transform=None):
        self.df = df.reset_index(drop=True)
        self.label_cols = label_cols
        self.transform = transform
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["file_loc"]).convert("RGB")
        if self.transform: img = self.transform(img)
        label = torch.tensor(int(row[self.label_cols[0]]), dtype=torch.long)
        return img, label, {"id_str": row["id_str"]}

class OversampleDataset(Dataset):
    def __init__(self, df, label_cols, transform=None, factor=8):
        self.base_ds = CatalogFrameDataset(df, label_cols, transform)
        self.factor = factor
        self.artifact_indices = [i for i, (_, _, meta) in enumerate(self.base_ds) if df.iloc[i]['morphology'] == 5]
        self.indices = list(range(len(self.base_ds)))
        self.indices += self.artifact_indices * (factor - 1)
    def __len__(self): return len(self.indices)
    def __getitem__(self, idx):
        return self.base_ds[self.indices[idx]]

def zoobot_collate(batch):
    imgs = torch.stack([x[0] for x in batch])
    labels = torch.stack([x[1] for x in batch])
    ids = [x[2]["id_str"] for x in batch]
    return {'image': imgs, 'morphology': labels, 'id_str': ids}

def build_dataloaders(train_df, val_df, test_df, batch_size=64, oversample=False):
    norm_mean = [0.485, 0.456, 0.406]
    norm_std = [0.229, 0.224, 0.225]
    train_tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean=norm_mean, std=norm_std)
    ])
    eval_tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=norm_mean, std=norm_std)
    ])
    TrainDS = OversampleDataset if oversample else CatalogFrameDataset
    train_loader = DataLoader(TrainDS(train_df, ["morphology"], train_tf), batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=zoobot_collate)
    val_loader = DataLoader(CatalogFrameDataset(val_df, ["morphology"], eval_tf), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=zoobot_collate)
    test_loader = DataLoader(CatalogFrameDataset(test_df, ["morphology"], eval_tf), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=zoobot_collate)
    return train_loader, val_loader, test_loader

# =========================================
# SECTION 5: FOCAL LOSS
# =========================================
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction='none')
    def forward(self, logits, targets):
        ce_loss = self.ce(logits, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()

# =========================================
# SECTION 6: MODEL WITH FOCAL & DROPOUT (Fixed)
# =========================================
class ZoobotWithFocalDropout(FinetuneableZoobotClassifier):
    def __init__(self, use_focal=False, dropout_prob=0.3, **kwargs):
        super().__init__(**kwargs)

        self.use_focal = use_focal # Keep this flag if you want

        # This is the magic: Overwrite the base class's criterion
        if use_focal:
            alpha = torch.tensor([0.25]*6)
            alpha[5] = 0.75 # Give 'Artifact' more weight

            # Note: The criterion will be moved to the correct device
            # by PyTorch Lightning automatically.
            self.criterion = FocalLoss(gamma=2.0, alpha=alpha)

        # Your custom head logic is perfect
        if hasattr(self.head, 'weight'):
            in_features = self.head.in_features
            self.head = nn.Sequential(
                nn.Dropout(p=dropout_prob),
                nn.Linear(in_features, self.num_classes)
            )

    # --- DO NOT ADD training_step or validation_step ---
    # The base class will now automatically use your FocalLoss.

# =========================================
# SECTION 7: CALLBACKS
# =========================================
class MetricsLogger(Callback):
    def __init__(self, csv_path):
        super().__init__()
        self.csv_path = csv_path
        self.rows = []
        self.cols = set()
    def on_validation_end(self, trainer, pl_module):
        m = trainer.callback_metrics
        row = {"epoch": trainer.current_epoch}
        for k, v in m.items():
            if isinstance(v, torch.Tensor): row[k] = float(v.cpu().item())
            elif isinstance(v, (int, float)): row[k] = v
        self.cols.update(row.keys())
        self.rows.append(row)
    def on_train_end(self, trainer, pl_module):
        os.makedirs(Path(self.csv_path).parent, exist_ok=True)
        cols = sorted(list(self.cols - {'epoch'}))
        final_cols = ['epoch'] + cols
        with open(self.csv_path, 'w', newline='') as f:
            w = csv.DictWriter(f, fieldnames=final_cols)
            w.writeheader()
            for r in self.rows:
                w.writerow({c: r.get(c, "") for c in final_cols})

# =========================================
# SECTION 8: 3 ABLATION RUNS
# =========================================
ablation_configs = [
    {"name": "oversample_only", "oversample": True,  "focal": False},
    {"name": "focal_loss_only", "oversample": False, "focal": True},
    {"name": "both",           "oversample": True,  "focal": True},
]

for cfg in ablation_configs:
    run_id = f"{cfg['name']}_{datetime.now():%Y%m%d_%H%M%S}"
    exp_dir = SAVE_ROOT / run_id
    exp_dir.mkdir(parents=True, exist_ok=True)

    # DataLoaders
    train_loader, val_loader, test_loader = build_dataloaders(
        train_df, val_df, test_df,
        batch_size=config['batch_size'],
        oversample=cfg['oversample']
    )

    # Model
    model = ZoobotWithFocalDropout(
        name=config['model_name'],
        num_classes=config['num_classes'],
        label_col='morphology',
        learning_rate=config['learning_rate'],
        layer_decay=config['layer_decay'],
        dropout_prob=config['dropout_prob'],
        training_mode=config['training_mode'],
        use_focal=cfg['focal']
    )

    # Callbacks
    ckpt_cb = ModelCheckpoint(dirpath=exp_dir/"checkpoints", filename="best", monitor="finetuning/val_loss", mode="min", save_top_k=1)
    es_cb = EarlyStopping(monitor="finetuning/val_loss", patience=config['early_stopping_patience'], min_delta=0.001)
    metrics_cb = MetricsLogger(csv_path=exp_dir / "metrics.csv")

    # Trainer
    trainer = L.Trainer(
        default_root_dir=str(exp_dir),
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices=1,
        max_epochs=config['max_epochs'],
        precision='16-mixed',
        callbacks=[ckpt_cb, es_cb, metrics_cb],
        enable_progress_bar=True,
        logger=False
    )

    print(f"\n{'='*60}")
    print(f"RUN: {cfg['name'].upper()} | Oversample: {cfg['oversample']} | Focal: {cfg['focal']}")
    print(f"{'='*60}")
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)

    # Load best model
    if ckpt_cb.best_model_path:
        model = ZoobotWithFocalDropout.load_from_checkpoint(
            ckpt_cb.best_model_path,
            name=config['model_name'],
            num_classes=config['num_classes'],
            label_col='morphology'
        )

    # Test
    test_results = trainer.test(model, dataloaders=test_loader, verbose=False)[0]
    test_acc = test_results.get('finetuning/test_acc', 0.0)

    # Predictions
    all_logits, all_preds, all_labels, all_ids = [], [], [], []
    device = trainer.strategy.root_device
    model.to(device)
    model.eval()
    with torch.no_grad():
        for batch in test_loader:
            imgs = batch['image'].to(device)
            labels = batch['morphology'].to(device)
            ids = batch['id_str']
            logits = model(imgs)
            all_logits.append(logits.cpu())
            all_preds.append(logits.argmax(1).cpu())
            all_labels.append(labels.cpu())
            all_ids.extend(ids)
    logits = torch.cat(all_logits)
    preds = torch.cat(all_preds)
    labels = torch.cat(all_labels)

    # Confusion Matrix
    cm = ConfusionMatrix(task="multiclass", num_classes=6)(preds, labels).numpy()
    plt.figure(figsize=(10,8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
    plt.ylabel('True'); plt.xlabel('Predicted'); plt.title(f'Confusion Matrix - {cfg["name"]}')
    plt.tight_layout(); plt.savefig(exp_dir / "confusion_matrix.png", dpi=200); plt.close()

    # Classification Report
    report = classification_report(labels.numpy(), preds.numpy(), target_names=CLASS_NAMES, output_dict=True)
    with open(exp_dir / "classification_report.txt", 'w') as f: f.write(classification_report(labels.numpy(), preds.numpy(), target_names=CLASS_NAMES))
    print(classification_report(labels.numpy(), preds.numpy(), target_names=CLASS_NAMES))

    # AUROC
    auroc = AUROC(task="multiclass", num_classes=6, average=None)
    scores = auroc(logits, labels).numpy()
    for name, score in zip(CLASS_NAMES, scores):
        print(f"{name:15}: AUROC = {score:.4f}")

    plt.figure(figsize=(10,6))
    sns.barplot(x=scores, y=CLASS_NAMES)
    plt.xlabel('AUROC'); plt.title(f'AUROC per Class - {cfg["name"]}'); plt.xlim(0,1)
    plt.tight_layout(); plt.savefig(exp_dir / "auroc_per_class.png", dpi=200); plt.close()

    # Learning Curve
    try:
        df = pd.read_csv(exp_dir / "metrics.csv")
        plt.figure(figsize=(10,6))
        plt.plot(df['epoch'], df['finetuning/val_loss'], 'o-', label='Val Loss')
        plt.plot(df['epoch'], df['finetuning/train_loss'], 's--', label='Train Loss')
        plt.title(f'Learning Curve - {cfg["name"]}'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)
        plt.tight_layout(); plt.savefig(exp_dir / "learning_curve.png", dpi=200); plt.close()
    except: pass

    # Save Predictions
    pred_df = pd.DataFrame({
        "id_str": all_ids, "true": labels.numpy(), "pred": preds.numpy(),
        "true_name": [CLASS_DICT[i] for i in labels.numpy()],
        "pred_name": [CLASS_DICT[i] for i in preds.numpy()]
    })
    probs = torch.softmax(logits, dim=1).numpy()
    for i, name in enumerate(CLASS_NAMES):
        pred_df[f"prob_{name}"] = probs[:, i]
    pred_df.to_csv(exp_dir / "test_predictions.csv", index=False)

    # README
    readme = f"""
# StarGPT Ablation: {cfg['name']}
**Run:** `{run_id}` | **Test Acc:** `{test_acc:.4f}` | **Mean AUROC:** `{np.mean(scores):.4f}`

## AUROC
"""
    for n, s in zip(CLASS_NAMES, scores):
        readme += f"- **{n}**: `{s:.4f}`\n"
    readme += f"\n## Files\n- `config.json`\n- `metrics.csv`\n- `confusion_matrix.png`\n- `test_predictions.csv`\n"
    with open(exp_dir / "README.md", 'w') as f: f.write(readme)

    # Save config
    run_config = {**config, **cfg, "test_accuracy": test_acc}
    with open(exp_dir / "config.json", 'w') as f:
        json.dump(run_config, f, indent=4)

print(f"\nALL 3 ABLATION RUNS COMPLETE!")
print(f"Results saved in: {SAVE_ROOT}")