# ========================================================
# STAR GPT: Galaxy Morphology Classification with Zoobot
# ECCE 635/787 Deep Learning Systems Design Project
# | UAE | November 11, 2025
# @title BaseLine Run - 172k Samples - 50 Epochs
# ========================================================
from google.colab import drive
drive.mount('/content/drive')

# =========================================
# SECTION 0: IMPORTS (100% WORKING)
# =========================================
import os, json, csv, logging, warnings
from pathlib import Path
from datetime import datetime
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# LIGHTNING v2+ → USE lightning.pytorch
import lightning.pytorch as L
from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, Callback
from lightning.pytorch import seed_everything

# ZOOBOT v2.9.0 → CORRECT IMPORT
from zoobot.pytorch.training import finetune
FinetuneableZoobotClassifier = finetune.FinetuneableZoobotClassifier

# TORCHMETRICS
from torchmetrics.classification import ConfusionMatrix, AUROC

# DATASET
from datasets import load_dataset

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("StarGPT")
seed_everything(42)

CLASS_NAMES = ['Smooth', 'Edge-on', 'Barred Spiral', 'Unbarred Spiral', 'Featured', 'Artifact']
CLASS_DICT = {i: name for i, name in enumerate(CLASS_NAMES)}

# =========================================
# SECTION 1: CONFIGURATION
# =========================================
config = {
    "run_name": "baseline_run",
    "description": "ConvNeXt-Base, 50k samples, 30 epochs, full finetuning",
    "use_colab": os.path.exists('/content'),
    "base_dir_colab": '/content/drive/My Drive/Colab_Results',
    "base_dir_local": str(Path.home() / 'zoobot_experiments'),
    "max_samples": 172000,
    "val_ratio": 0.10,
    "test_ratio": 0.20,
    "random_state": 42,
    "model_name": "hf_hub:mwalmsley/zoobot-encoder-convnext_base",
    "num_classes": 6,
    "dropout_prob": 0.3,
    "batch_size": 128,
    "learning_rate": 5e-5,
    "layer_decay": 0.8,
    "max_epochs": 50,
    "early_stopping_patience": 3,
    "training_mode": "full"
}

# Paths
USE_COLAB = config['use_colab']

# --- NEW PATHS ---
# 1. Set SAVE_ROOT to your Google Drive for permanent results
SAVE_BASE_DIR = Path(config['base_dir_colab'] if USE_COLAB else config['base_dir_local'])
SAVE_ROOT = SAVE_BASE_DIR / 'experiment_runs'

# 2. Set DATA_DIR to the fast temporary Colab disk for images
# This data will be deleted when the session ends, which is fine!
DATA_BASE_DIR = Path('/content/gz2_temp_data') if USE_COLAB else (Path.home() / 'gz2_temp_data')
DATA_DIR = DATA_BASE_DIR / 'gz2_data'

# 3. Define the specific experiment directory on your Drive
RUN_ID = f"{config['run_name']}_{datetime.now():%Y%m%d_%H%M%S}"
EXP_DIR = SAVE_ROOT / RUN_ID

# Create all necessary directories
for p in [SAVE_BASE_DIR, SAVE_ROOT, EXP_DIR, DATA_BASE_DIR, DATA_DIR]:
    p.mkdir(parents=True, exist_ok=True)

print(f"Data Path (Temporary Colab Disk): {DATA_DIR}")
print(f"Save Path (Permanent Google Drive): {EXP_DIR}")

# Save config
with open(EXP_DIR / "config.json", 'w') as f:
    json.dump(config, f, indent=4)

# =========================================
# SECTION 2: DATA LOADER
# =========================================
def assign_morphology_class(ex):
    s = ex.get('smooth-or-featured-gz2_smooth_fraction', 0.0)
    f = ex.get('smooth-or-featured-gz2_featured-or-disk_fraction', 0.0)
    e = ex.get('disk-edge-on-gz2_yes_fraction', 0.0)
    sp = ex.get('has-spiral-arms-gz2_yes_fraction', 0.0)
    b = ex.get('bar-gz2_yes_fraction', 0.0)
    if s > 0.5: return 0
    elif f > 0.5:
        if e > 0.5: return 1
        elif sp > 0.5: return 2 if b > 0.5 else 3
        else: return 4
    else: return 5

def prepare_gz2_dataset_multiclass(data_dir: Path, max_samples=5000):
    logger.info("Streaming GZ2 dataset...")
    ds = load_dataset("mwalmsley/gz2", split="train", streaming=True)
    os.makedirs(data_dir, exist_ok=True)
    recs = []
    for i, ex in enumerate(ds.take(max_samples)):
        if i % (max_samples // 10) == 0: logger.info(f"Processing {i}/{max_samples}")
        gid = str(ex.get('id_str', f'gal_{i}')).replace('.jpg', '')
        img_path = data_dir / f"{gid}.jpg"
        if 'image' in ex and not img_path.exists():
            try: ex['image'].save(img_path)
            except: continue
        if not img_path.exists(): continue
        cls = assign_morphology_class(ex)
        recs.append({"id_str": gid, "file_loc": str(img_path), "morphology": int(cls)})
    df = pd.DataFrame(recs)
    logger.info(f"Classes:\n{df['morphology'].value_counts().sort_index().rename(CLASS_DICT)}")
    return df

# =========================================
# SECTION 3: LOAD & SPLIT
# =========================================
print("="*80)
print(f"Loading dataset (Max Samples: {config['max_samples']})...")
print("="*80)
full_df = prepare_gz2_dataset_multiclass(DATA_DIR, max_samples=config['max_samples'])
n_samples = min(config['max_samples'], len(full_df))
capped_df = full_df.sample(n=n_samples, random_state=config['random_state']).reset_index(drop=True)

train_val_df, test_df = train_test_split(capped_df, test_size=config['test_ratio'], stratify=capped_df['morphology'], random_state=config['random_state'])
train_df, val_df = train_test_split(train_val_df, test_size=config['val_ratio'], stratify=train_val_df['morphology'], random_state=config['random_state'])

train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print(f"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")
print(f"Leakage: {len(set(train_df.id_str) & set(test_df.id_str))}")

# =========================================
# SECTION 4: DATALOADER
# =========================================
class CatalogFrameDataset(Dataset):
    def __init__(self, df, label_cols, transform=None):
        self.df = df.reset_index(drop=True)
        self.label_cols = label_cols
        self.transform = transform
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row["file_loc"]).convert("RGB")
        if self.transform: img = self.transform(img)
        label = torch.tensor(int(row[self.label_cols[0]]), dtype=torch.long)
        return img, label, {"id_str": row["id_str"]}

def zoobot_collate(batch):
    imgs = torch.stack([x[0] for x in batch])
    labels = torch.stack([x[1] for x in batch])
    ids = [x[2]["id_str"] for x in batch]
    return {'image': imgs, 'morphology': labels, 'id_str': ids}

def build_dataloaders(train_df, val_df, test_df, batch_size=32):
    norm_mean = [0.485, 0.456, 0.406]
    norm_std = [0.229, 0.224, 0.225]
    train_tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(15),
        transforms.ToTensor(),
        transforms.Normalize(mean=norm_mean, std=norm_std)
    ])
    eval_tf = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=norm_mean, std=norm_std)
    ])
    train_loader = DataLoader(CatalogFrameDataset(train_df, ["morphology"], train_tf), batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=zoobot_collate)
    val_loader = DataLoader(CatalogFrameDataset(val_df, ["morphology"], eval_tf), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=zoobot_collate)
    test_loader = DataLoader(CatalogFrameDataset(test_df, ["morphology"], eval_tf), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=zoobot_collate)
    return train_loader, val_loader, test_loader

# =========================================
# SECTION 5: MODEL
# =========================================
class ZoobotWithDropout(FinetuneableZoobotClassifier):
    def __init__(self, dropout_prob=0.3, **kwargs):
        super().__init__(**kwargs)
        if hasattr(self.head, 'weight'):
            in_features = self.head.in_features
            out_features = self.head.out_features
            self.head = nn.Sequential(
                nn.Dropout(p=dropout_prob),
                nn.Linear(in_features, out_features)
            )

# =========================================
# SECTION 6: CALLBACKS
# =========================================
class MetricsLogger(Callback):
    def __init__(self, csv_path):
        super().__init__()
        self.csv_path = csv_path
        self.rows = []
        self.cols = set()
    def on_validation_end(self, trainer, pl_module):
        m = trainer.callback_metrics
        row = {"epoch": trainer.current_epoch}
        for k, v in m.items():
            if isinstance(v, torch.Tensor): row[k] = float(v.cpu().item())
            elif isinstance(v, (int, float)): row[k] = v
        self.cols.update(row.keys())
        self.rows.append(row)
    def on_train_end(self, trainer, pl_module):
        os.makedirs(Path(self.csv_path).parent, exist_ok=True)
        cols = sorted(list(self.cols - {'epoch'}))
        final_cols = ['epoch'] + cols
        with open(self.csv_path, 'w', newline='') as f:
            w = csv.DictWriter(f, fieldnames=final_cols)
            w.writeheader()
            for r in self.rows:
                w.writerow({c: r.get(c, "") for c in final_cols})

# =========================================
# SECTION 7: TRAIN
# =========================================
train_loader, val_loader, test_loader = build_dataloaders(train_df, val_df, test_df, batch_size=config['batch_size'])

model = ZoobotWithDropout(
    name=config['model_name'],
    num_classes=config['num_classes'],
    label_col='morphology',
    learning_rate=config['learning_rate'],
    layer_decay=config['layer_decay'],
    dropout_prob=config['dropout_prob'],
    training_mode=config['training_mode']
)

ckpt_cb = ModelCheckpoint(
    dirpath=EXP_DIR / "checkpoints",
    filename="best",
    monitor="finetuning/val_loss",
    mode="min",
    save_top_k=1
)
es_cb = EarlyStopping(monitor="finetuning/val_loss", patience=config['early_stopping_patience'], min_delta=0.001)
metrics_cb = MetricsLogger(csv_path=EXP_DIR / "metrics.csv")

trainer = L.Trainer(
    default_root_dir=str(EXP_DIR),
    accelerator="gpu" if torch.cuda.is_available() else "cpu",
    devices=1,
    max_epochs=config['max_epochs'],
    precision='16-mixed',
    callbacks=[ckpt_cb, es_cb, metrics_cb],
    enable_progress_bar=True,
    logger=False
)

print(f"\nTraining StarGPT on {len(train_df)} galaxies...")
trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)

# =========================================
# SECTION 8: EVALUATION
# =========================================
print("\nEvaluating on test set...")
if ckpt_cb.best_model_path:
    model = ZoobotWithDropout.load_from_checkpoint(
        ckpt_cb.best_model_path,
        name=config['model_name'],
        num_classes=config['num_classes'],
        label_col='morphology'
    )

test_results = trainer.test(model, dataloaders=test_loader, verbose=False)[0]
test_acc = test_results.get('finetuning/test_acc', 0.0)

# Collect predictions
all_logits, all_preds, all_labels, all_ids = [], [], [], []
device = trainer.strategy.root_device

model.to(device)
model.eval()

with torch.no_grad():
    for batch in test_loader:
        imgs = batch['image'].to(device)
        labels = batch['morphology'].to(device)
        ids = batch['id_str']
        logits = model(imgs)
        all_logits.append(logits.cpu())
        all_preds.append(logits.argmax(1).cpu())
        all_labels.append(labels.cpu())
        all_ids.extend(ids)

logits = torch.cat(all_logits)
preds = torch.cat(all_preds)
labels = torch.cat(all_labels)

# Confusion Matrix
cm = ConfusionMatrix(task="multiclass", num_classes=6)(preds, labels).numpy()
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
plt.ylabel('True'); plt.xlabel('Predicted'); plt.title('Confusion Matrix')
plt.tight_layout(); plt.savefig(EXP_DIR / "confusion_matrix.png", dpi=200); plt.show()

# Classification Report
report = classification_report(labels.numpy(), preds.numpy(), target_names=CLASS_NAMES)
with open(EXP_DIR / "classification_report.txt", 'w') as f: f.write(report)
print(report)

# AUROC
auroc = AUROC(task="multiclass", num_classes=6, average=None)
scores = auroc(logits, labels).numpy()
for name, score in zip(CLASS_NAMES, scores):
    print(f"{name:15}: AUROC = {score:.4f}")

# AUROC Plot
plt.figure(figsize=(10,6))
sns.barplot(x=scores, y=CLASS_NAMES)
plt.xlabel('AUROC'); plt.title('AUROC per Class'); plt.xlim(0,1)
plt.tight_layout(); plt.savefig(EXP_DIR / "auroc_per_class.png", dpi=200); plt.show()

# Learning Curve
try:
    df = pd.read_csv(EXP_DIR / "metrics.csv")
    plt.figure(figsize=(10,6))
    plt.plot(df['epoch'], df['finetuning/val_loss'], 'o-', label='Val Loss')
    plt.plot(df['epoch'], df['finetuning/train_loss'], 's--', label='Train Loss')
    plt.title('Learning Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)
    plt.tight_layout(); plt.savefig(EXP_DIR / "learning_curve.png", dpi=200); plt.show()
except: pass

# Save Predictions
pred_df = pd.DataFrame({
    "id_str": all_ids,
    "true": labels.numpy(),
    "pred": preds.numpy(),
    "true_name": [CLASS_DICT[i] for i in labels.numpy()],
    "pred_name": [CLASS_DICT[i] for i in preds.numpy()]
})
probs = torch.softmax(logits, dim=1).numpy()
for i, name in enumerate(CLASS_NAMES):
    pred_df[f"prob_{name}"] = probs[:, i]
pred_df.to_csv(EXP_DIR / "test_predictions.csv", index=False)

# =========================================
# SECTION 9: README
# =========================================
try:
    df = pd.read_csv(EXP_DIR / "metrics.csv")
    best_val = df['finetuning/val_loss'].min()
    final_epoch = df['epoch'].max()
except: best_val, final_epoch = 0.0, 0

readme = f"""
# StarGPT Results: {config['run_name']}
**Run:** `{RUN_ID}` | **Test Acc:** `{test_acc:.4f}` | **Mean AUROC:** `{np.mean(scores):.4f}`

## AUROC
"""
for n, s in zip(CLASS_NAMES, scores):
    readme += f"- **{n}**: `{s:.4f}`\n"

readme += f"\n## Files\n- `config.json`\n- `metrics.csv`\n- `confusion_matrix.png`\n- `test_predictions.csv`\n"

with open(EXP_DIR / "README.md", 'w') as f: f.write(readme)

print(f"\nSTAR GPT COMPLETE | Results: {EXP_DIR}")